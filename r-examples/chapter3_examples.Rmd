---
title: "Chapter 3 Examples"
output: html_notebook
---

***

### Example 3-2-1

To obtain the sample covariance for the height and weight data in Table 3.1,
we first calculate $x$, $y$, and $\sum_{i}x_{i}y{i}$, where $x$ is height and $y$ is weight:

```{r}
#defining vectors x and y
x=c(69,74,68,70,72,67,66,70,76,68,72,79,74,67,66,71,74,75,75,76)
y=c(153,175,155,135,172,150,115,137,200,130,140,265,
    185,112,140,150,165,185,210,220)
xbar=mean(x)
ybar=mean(y)

#printing
x
y
xbar
ybar
```

```{r}
#defining covariance between x and y
Sxy=cov(x,y)
Sxy

#rounding covariance to two decimals
RoSxy=round(Sxy,2)
RoSxy
```
By itself, the sample covariance $128.88$ is not very meaningful. We are not sure if this represents a small, moderate, or large amount of relationship between $y$ and $x$. A method of standarizing the covariance is given in the next section.

```{r}
#plot
par(mfrow=c(2,2))
plot(x)
plot(x,type="l")
plot(y)
plot(y,type="l")
```


### Example 3-2-2

To obtain the correlation for the height and weight data of Table 3.1, we first calculate the sample variance of $x$ and $y$:

**Variances**
```{r}
S2x=var(x)
S2x
RoS2x=round(S2x,3)
RoS2x
```


```{r}
S2y=var(y)
S2y
RoS2y=round(S2y,3)
RoS2y
```


**Standard Deviation**

Then $s_{x}$ and $s_{y}$ equal:
```{r}
Sx=sqrt(S2x)
Sx
RoSx=round(Sx,4)
RoSx

Sy=sqrt(S2y)
Sy
RoSy=round(Sy,3)
RoSy

```

**Correlation between $x$ and $y$**

And finally we get $r_{xy}=\frac{s_{xy}}{s_{x}s_{y}}$, which is equal to:
```{r}
r=cor(x,y)
r
Ror=round(r,3)
Ror
```


### Example 3-6

Table 3.4 gives partial data from Kramer and Jensen (1969a). Three variables were measured (in milliequivalents per 100 g) at 10 different locations in the South. The variables are:


+ $y_{1}$= available soil calcium

+ $y_{2}$= exchangeanble soil calcium

+ $y_{3}$= turnip green calcium

```{r}

#defining "y" matrix (10 x 3)
y=matrix(c(35,35,40,10,6,20,35,35,35,30,3.5,4.9,30,2.8,2.7,2.8,4.6,10.9,
           8,1.6,2.8,2.7,4.38,3.21,2.73,2.81,2.88,2.9,3.28,3.2),10)
y
```

To obtain the mean vector $\bar{y}$, we simply calculate the average of each column and obtain:

```{r}
#calcula el promedio por columna y luego toma los argumentos del vector renglÃ³n, transformandolo en un vector columna (con "cbind")
ybar=cbind(apply(y,2,FUN=mean))
#apply(X, MARGIN, FUN, ...)
#MARGIN: a vector giving the subscripts which the function will be applied over. E.g., for a matrix 1 indicates rows, 2 indicates columns, c(1, 2) indicates rows and columns. Where X has named dimnames, it can be a character vector selecting dimension names.
ybar
```


```{r}
#se le aplica la traspuesta ("t") a ybar
Tybar=t(ybar)
Tybar
```

### Example 3-7


To calculate the sample covariance matrix for the calcium data of Table 3.4 using the computational forms (3.24) and (3.26), we need the sum of squares of each column and the sum of products of each pair of columns. We illustrate the computation of $s_{13}$


$\sum_{i=1}^{10}y_{i1}y_{i3}=(35)(2.80)+(35)(2.70)+...=885.48$

From Example 3.6, we have $\bar{y_{1}}=28.1$ and $\bar{y_{3}}=3.089$. By (3.26), we obtain $s{13}$=$\frac{1}{10-1}[885.48-10(28.1)(3.089)]=1.91412$$


Continuing in this fashion, we obtain $\textbf{S}$:

```{r}
#estimating the covariance of "y"
S=cov(y)
S
RoS=round(S,2)
RoS
```

### Example 3-8

In Example 3.7, we obtained the sample covariance matrix $\textbf{S}$ for the calcium data in Table 3.4. To obtain the sample correlation matrix for the same data, we can calculate the individual elements using (3.34) or use the direct matrix operation in (3.37). The diagonal matrix $\textbf{D}_{s}$ s can be found by taking the square roots of the diagonal elements of $\textbf{S}$,

```{r}
#estimating the correlation of y
R=cor(y)
R
RoR=round(R,3)
RoR
#the diagnonal of the covariance matrix of "y"
d=diag(diag(S))
d
#root square of matrix d
D=sqrt(d)
D

```

(note that we have used the unrounded version of $\textbf{S}$ for computation). Then by (3.37),

$$\textbf{R}=\textbf{D}_{s}^{-1}\textbf{S}\textbf{D}_{s}^{-1}$$

```{r}
DI=solve(D)
DI

R=DI%*%S%*%DI
R
RoR=round(R,3)
RoR

```

Note that $.865 > .493 > .327$, which is a different order than that of the covariances in $\textbf{S}$ in Example 3.7. Thus we cannot compare covariances, even within the same matrix $\textbf{S}$.

```{r}
S=D%*%R%*%D
S
RoS=round(S,2)
RoS
```

### Example 3-9-1

Reaven and Miller (1979; see also Andrews and Herzberg 1985, pp. 215-219) measured five variables in a comparison of normal patients and diabetics. In Table 3.5 we give partial data for normal patients only. The three variables of major interest were:

+ $x_{1}$= glucose intolerance
+ $x_{2}$= insuline response to oral glucose
+ $x_{3}$= insuline resistance

The two aditional variables of minor interest were:

+ $y_{1}$= relative weights
+ $y_{2}$= fasting plasma glucose


```{r}
x=matrix(c(356,289,319,356,323,381,350,301,379,296,353,306,290,371,
           312,393,364,359,296,345,378,304,347,327,386,365,365,352,325,321,360,
           336,352,353,373,376,367,335,396,277,378,360,291,269,318,328,124,117,
           143,199,240,157,221,186,142,131,221,178,136,200,208,202,152,185,116,
           123,136,134,184,192,279,228,145,172,179,222,134,143,169,263,174,134,
           182,241,128,222,165,282,94,121,73,106,55,76,105,108,143,165,119,105,
           98,94,53,66,142,93,68,102,76,37,60,50,47,50,91,124,74,235,158,140,
           145,99,90,105,32,165,78,80,54,175,80,186,117,160,71,29,42,56),46)
x
```


```{r}
y=matrix(c(0.81,0.95,0.94,1.04,1,0.76,0.91,1.1,0.99,0.78,0.90,
           0.73,0.96,0.84,0.74,0.98,1.1,0.85,0.83,0.93,0.95,
           0.74,0.95,0.97,0.72,1.11,1.2,1.13,1,0.78,1,1,0.71,0.76,0.89,0.88,1.17,
           0.85,0.97,1,1,0.89,0.98,0.78,0.74,0.91,80,97,105,90,90,86,100,85,
           97,97,91,87,78,90,86,80,90,99,85,90,90,88,95,90,92,74,98,100,86,98,
           70,99,75,90,85,99,100,78,106,98,102,90,94,80,93,86),46)
y
```

```{r}
Y=cbind(y,x)
Y
```


The mean vector, partitioned as in (3.41), is:

$$ \begin{pmatrix}
\bar{y}\\
\bar{x}
\end{pmatrix} = \begin{pmatrix}
\frac{\bar{y}_{1}\\
\bar{y}_{2}\\}
{\bar{x}_{1}\\
\bar{x}_{2}\\
\bar{x}_{3}\\}
\end{pmatrix}$$

```{r}
Ybar=cbind(apply(Y,2,FUN=mean))
Ybar
RoYbar=round(Ybar,2)
RoYbar
```

The covariance matrix, partitioned as in the illustration following (3.44) is:


```{r}
S=matrix(var(Y),5)
S
RoS=round(S,4)
RoS
```


Note that $S_{yy}$ and $S_{xx}$ are symmetric and that $S_{xy}$ is the transpose of $S_{yx}$

```{r}
W=matrix(c(0.81,0.95,0.94,1.04,1,0.76,0.91,1.1,0.99,0.78,0.90,
           0.73,0.96,0.84,0.74,0.98,1.1,0.85,0.83,0.93,0.95,
           0.74,0.95,0.97,0.72,1.11,1.2,1.13,1,0.78,1,1,0.71,0.76,0.89,0.88,1.17,
           0.85,0.97,1,1,0.89,0.98,0.78,0.74,0.91,80,97,105,90,90,86,100,85,
           97,97,91,87,78,90,86,80,90,99,85,90,90,88,95,90,92,74,98,100,86,98,
           70,99,75,90,85,99,100,78,106,98,102,90,94,80,93,86,
           356,289,319,356,323,381,350,301,379,296,353,306,290,371,
           312,393,364,359,296,345,378,304,347,327,386,365,365,352,325,321,360,
           336,352,353,373,376,367,335,396,277,378,360,291,269,318,328,124,117,
           143,199,240,157,221,186,142,131,221,178,136,200,208,202,152,185,116,
           123,136,134,184,192,279,228,145,172,179,222,134,143,169,263,174,134,
           182,241,128,222,165,282,94,121,73,106,55,76,105,108,143,165,119,105,
           98,94,53,66,142,93,68,102,76,37,60,50,47,50,91,124,74,235,158,140,
           145,99,90,105,32,165,78,80,54,175,80,186,117,160,71,29,42,56),46)
W

Wbar=cbind(apply(W,2,FUN=mean))
Wbar
RoWbar=round(Wbar,2)
RoWbar

SW=matrix(var(W),5)
SW
RoSW=round(SW,4)
RoSW
```



### Example 3-10-1


Timm (1975, p. 233; 1980, p. 47) reported the results of an experiment in which subjects responded to "probe words" at five positions in a sentence. The variables are response times for the jth probe word, $y_{j},j = 1,2,..., 5$. The data are given by $Y$.


```{r}
y=matrix(c(51,27,37,42,27,43,41,38,36,26,29,36,20,22,36,18,32,22,21,
           23,31,20,50,26,41,32,33,43,36,31,27,31,25,35,17,37,34,14,35,25,20,25,32,26,42,27,30,27,29,40,38,16,28,36,25),11)
y
```

```{r}
y=matrix(c(51,27,37,42,27,43,41,38,36,26,29,36,20,22,36,18,32,22,21,
           23,31,20,50,26,41,32,33,43,36,31,27,31,25,35,17,37,34,14,35,25,20,25,
           32,26,42,27,30,27,29,40,38,16,28,36,25),11)
y
```

These variables are commensurate (same measurement units and similar means and variances), and the researcher may wish to examine some simple linear combinations. Consider the following linear combination for illustrative purposes:


$$z=3y_{1}-2y_{2}+4y_{3}-y{4}+y_{5}$$

$$=(3, -2, 4, -1, 1)\textbf{y}= \textbf{a'y}$$

If $z$ is calculated for each of the 11 observations, we obtain $z_{1}=288, z_{2} = 155, ... ,z_{11} = 146$ with mean $\bar{z} = 197.0$ and variance $s_{z}^2 = 2084.0$. 

These same results can be obtained using (3.54) and (3.55). The sample **mean vector** and **covariance matrix** for the data are:

$\bar{\textbf{y}}$:
```{r}
ybar=cbind(apply(y,2,FUN=mean))
ybar
Roybar=round(ybar,2)
Roybar
```

$\textbf{S}$:
```{r}
S=var(y)
S
RoS=round(S,2)
RoS
```

Then, by (3.54),

$$\bar{z}=\textbf{a'}\bar{y}$$

```{r}
a=cbind(c(3,-2,4,-1,1))
a

z=t(a)%*%t(y)
z

zbar=t(a)%*%ybar
zbar

```

and by (3.55), 

```{r}
s2z=t(a)%*%S%*%a
s2z

```


We now define a second linear combination:


$$w=y_{1}+3y_{2}-y_{3}+y_{4}-2y_{5}$$

$$=(1, 3, -1, 2, -2)\textbf{y}= \textbf{b'y}$$
The sample mean and variance of $w$ are $\bar{w}=\textbf{b'}\bar{y}=44.45$ and $s_{w}^2=\textbf{b'Sb}=605.67$. The sample covariance of $z$ and $w$ is by (3.56), $s_{zw}=\textbf{a'Sb}=40.2$.
```{r}
b=cbind(c(1,3,-1,1,-2))
b

w=t(b)%*%t(y)
w

wbar=t(b)%*%ybar
wbar
Rowbarb=round(wbar,2)
Rowbarb

s2w=t(b)%*%S%*%b
s2w
Ros2w=round(s2w,2)
Ros2w

Czw=t(a)%*%S%*%b
Czw
```


##### Another Form

```{r}
  f=function(y,a,b){
    ybar=cbind(apply(y,2,FUN=mean))
    Roybar=round(ybar,2)
    S=var(y)
    RoS=round(S,2)
    z=t(a)%*%t(y)
    w=t(b)%*%t(y)
    zbar=t(a)%*%ybar
    Rozbar=round(zbar,2)
    wbar=t(b)%*%ybar
    Rowbar=round(wbar,2)
    s2z=t(a)%*%S%*%a
    Ros2z=round(s2z,2)
    s2w=t(b)%*%S%*%a
    Ros2w=round(s2w,2)
    print(z)
    print(w)
    print(Roybar)
    print(RoS)
    print(Rozbar)
    print(Rowbar)
    print(Ros2z)
    print(Ros2w)
  }

y=matrix(c(51,27,37,42,27,43,41,38,36,26,29,36,20,22,36,18,32,22,21,
           23,31,20,50,26,41,32,33,43,36,31,27,31,25,35,17,37,34,14,35,25,20,25,
           32,26,42,27,30,27,29,40,38,16,28,36,25),11)

a=cbind(c(3,-2,4,-1,1))
b=cbind(c(1,3,-1,1,-2))
f(y,a,b)
```

We now define three linear functions:

$$z_{1}=y_{1}+y_{2}+y_{3}+y_{4}+y_{5}$$
$$z_{2}=2y_{1}-3y_{2}+y_{3}-2y_{4}-y_{5}$$

$$z_{3}=-y_{1}-2y_{2}+y_{3}-2y_{4}+3y_{5}$$
which can be written as:

$$\textbf{z}=\textbf{Ay}$$


```{r}
A=matrix(c(1,2,-1,1,-3,-2,1,1,1,1,-2,-2,1,-1,3),3)
A
S=var(y)
S

Z=A%*%t(y)
Z
```


and the sample mean vectorfor $\txtbf{z}$ is given by (3.62) as:

$$\bar{z}=\textbf{A}\bar{y}$$

```{r}
Zbar=cbind(apply(Z,1,FUN=mean))
Zbar
RoZbar=round(Zbar,2)
RoZbar
```

and the sample covariance matrix of $\txtbf{z}$ is given by (3.64) as:


$$S_{z}= \textbf{ASA'}$$
```{r}
Sz=A%*%S%*%t(A)
Sz
RoSz=round(Sz,2)
RoSz
```

The covariance matrix $$S_{x}$$, can be converted to a correlation matrix by use of (3.37):
$$R_{z}= \textbf{D}_{z}^{-1}\textbf{S}_{z}\textbf{D}_{z}^{-1}$$
```{r}

Sz=A%*%S%*%t(A)
Sz
RoSz=round(Sz,2)
RoSz


```


```{r}
Dz=diag(sqrt(diag(Sz)))
Dz
RoDz=round(Dz,2)
RoDz

IDz=solve(Dz)
IDz

Rz=IDz%*%Sz%*%IDz
Rz
RoRz=round(Rz,2)
RoRz
```

where $D_{z}$ is obtained from the square roots of the diagonal elements of $S_{z}$.

### Example 3-12

We illustrate the iterated regression method of estimating missing values. Consider the calcium data of Table 3.4 as reproduced here and suppose the entries in parentheses are missing:
```{r}

#defining "y" matrix
y=matrix(c(35,35,40,10,6,20,35,35,35,30,3.5,4.9,30,2.8,2.7,2.8,4.6,10.9,
           8,1.6,2.8,2.7,4.38,3.21,2.73,2.81,2.88,2.9,3.28,3.20),10)
y
```

```{r}
#mean per column and returning the result as a column vector.
ybar=cbind(apply(y,2,FUN=mean))
ybar
#roundig the results of ybar to three decimals
Roybar=round(ybar,3)
Roybar
```


```{r}
#keeping rows 3-10 from column 2
y[3:10,2]

```

We first regress $y_{2}$ on $y_{1}$ and $y_{3}$ for observations $3-10$ and obtain $\hat{y_{2}} = b_{0} + b_{1}y_{1} + b_{3}y_{3}$. 
```{r}

#lm is used to fit linear models. It can be used to carry out regression, single stratum analysis of variance and analysis of covariance (although aov may provide a more convenient interface for these). Inside the lm parenthesis we have to insert a formula, like the one provided below.
L2=lm(y[3:10,2]~y[3:10,1]+y[3:10,3])
L2
```

When this is evaluated for the two nonmissing entries in the first row ($y_{1}$= 35 and $y_{3}$ = 2.80), we obtain $\hat{y_{2}} = 4.097$. 

```{r}
yhat2=-39.9433+0.1542*y[1,1]+13.8011*y[1,3]
yhat2
Royhat2=round(yhat2,3)
Royhat2
```

Similarly, we regress $y_{3}$on $y_{1}$ and $y_{2}$ for observations $3-10$ to obtain $\hat{y_{3}} = c_{0} + c_{1}y_{1} + c_{2}y_{2}$. 

```{r}
L3=lm(y[3:10,3]~y[3:10,1]+y[3:10,2])
L3
```
Evaluating this for the two nonmissing entries in the second row yields $\hat{y_{3}}  = 3.011$. 

```{r}
yhat3=2.814291+-0.001377*y[2,1]+0.049941*y[2,2]
yhat3
Royhat3=round(yhat3,3)
Royhat3
```

We now insert these estimates for the missing values and calculate the regression equations based on all 10 observations. Using the revised equation $\hat{y_{2}} = b_{0} + b_{1}y_{1} + b_{3}y_{3}$, we obtain a new predicted value, $\hat{y_{2}}= 3.698$. 


Similarly, we obtain a revised regression equation for $y_{3}$ that gives a new predicted value, $\hat{y_{3}}  = 2.981$. 

With these values inserted, we calculate new equations and obtain new predicted values,$\hat{y_{2}}  = 3.672$ and $\hat{y_{3}}  = 2.976$. At the third iteration we obtain $\hat{y_{2}}  = 3.679$ and $\hat{y_{3}}  = 2.975$. There is very little change in subsequent iterations. 

These values are closer to the actual values, $y_{2} = 3.5$ and ${y_{3}}= 2.70$, than the initial regression estimates, $\hat{y_{2}}  = 4.097$ and $\hat{y_{3}}  = 3.011$. 

They are also much better estimates than the means of the second and third columns, $\bar{y_{2}}$ = 7.589 and $\overline{y_{3}}$ = 3.132.
